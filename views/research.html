<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-object-group fa-fw w3-margin-right w3-xxlarge w3-text-teal"></i>Research</h2>
  <div class="well">
    <h4>This section contains a series of blog posts about 3 areas of research that I am most interested in:</h4>
    <ul>
      <li>Natural Language Interface to DataBases (NLIDB)</li>
      <li>Word2vec</li>
      <li>BlockChain Technology</li>
    </ul>
    <p>The posts contain my annotations of the most current and relevant research papers as well as some information on experimentations that I conduct to test ideas in the areas of reseach listed above.</p>
  </div>
</div>

<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">September 14, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 2 (Word2vec)</h4>
    <p><span class="display-field">Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." Advances in neural information processing systems. 2013.</span></p>
    <p>Researchers at Google extend on the continuous Skip-gram model by using subsampling of frequent words from text data in order to improve the quality of the word vectors and the training speed. They also present a simplified variant of Noise Contrastive Estimation (NCE) that further improves the accuracy and speed of training. Part of the research seems to relate closely to thought vectors since it discusses extensions from word-based models to phrase-based models through a data-driven approach.</p>
    <p>In introducing the main idea, the paper discusses the statistical models used in the Skip-gram model. Google’s Skip-gram model is used to learn high quality vector representations of words from large amounts of unstructured text data. Given a sequence of training words the model maximizes the average log probability. The authors use a Binary Huffman Tree data structure in the Hierarchical Softmax model to assign short codes to frequent words so that words can be grouped together by their frequency. An alternative to Hierarchical Softmax, called Negative Sampling is presented, which is a simplified version of the Noise Contrastive Estimation (NCE) that uses only samples of the noise distribution (whilst NCE uses both the samples and the numerical probabilities of the noise distribution). Another significant extension to the Skip-gram model in this research paper is the subsampling of frequent words, whereby the model learns to counter the balance of rare (e.g. “France”) and frequent (e.g. “the”, “a”) words. Additive compositionality is also explained to be used in the phrase skip-gram model.
    </p>
    <p>The empirical results of the word vector tasks show that Negative Sampling outperformed Hierarchical Softmax and the Noise Contrastive Estimation on the analogical reasoning task. Furthermore, the subsampling of the frequent words improved the training speed. The results of the Skip-gram model for phrases shows that the subsampling can result in faster training and can also improve accuracy in some cases. </p>
    <p>From a critical perspective, the authors explain the conditions in their experiments well and also compare their results to results of other neural network based models of representing words. However, the authors are biased toward the Skip-gram model and it is interesting to ask how much this academic research is influenced by the objectives of Google. </p>
    <hr>
  </div>
</div>

<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">September 8, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 1 (NLIDBS)</h4>
    <p><span class="display-field">Singh, Garima, and Arun Solanki. "An algorithm to transform natural language into SQL queries for relational databases." Selforganizology 3.3 (2016): 110-126.</span></p>
    <p>
      This paper is an engineering and empirical research paper. The authors, researchers from Gautam Buddha University, study how to process complex natural language queries and minimize ambiguity by using set of production rules and data dictionary which consists of semantics sets for relations and attributes. They explain the process of converting a natural language sentence into a SQL query through a series of steps that include lowercase conversion, tokenization, removal of escape words (words that have significance in SQL), part of speech tagging (categorizing words as nouns, pronouns, or verbs), classification of elements (putting the tokens into relations, attributes and clauses), removal of ambiguous attributes, and finally building of the query itself. The authors also discuss the results of testing their prototype and compare them with the results of an older NLIDBS. However, they do not provide any information about the older NLIDBS and the results generated for it. They also fail to clearly state what exactly their hypothesis is and what they are testing. This paper will be helpful in understanding the implementations of an NLIDBS, but there seems to be no clear evidence of the unique contribution to the field brought forward by this paper.
    </p>
    <h4 class="w3-text-teal" style="margin:30px auto;">AIML Test app</h4>
    <p>I implemented a <a href="https://github.com/Bovojon/AIML-test-app">Chatbot</a> using AIML-Python by following this amazing online <a href="http://www.devdungeon.com/content/ai-chat-bot-python-aiml" target="_blank">tutorial</a>. AIML (Artificial Intelligence Markup Language) is an XML-compliant language used for creating or customizing an Alicebot. I will be customizing and adding more functionality to the <a href="https://github.com/Bovojon/AIML-test-app">Chatbot</a> overtime.</p>
    <hr>
  </div>
</div>


<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">September 4, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Introduction of ideas</h4>
    <h4 class="w3-opacity"><b>Natural Language Interface to Databases (NLIDB)</b></h4>
    <p>I am curious to learn how Natural Language Processing can be applied to the process of interacting with databases – SQL and NoSQL. One of goals for this research project is to learn how SQL can be democratized so that relational databases can be written to and queried in natural language. I can see huge potential in such systems where one can query a database system using natural language - they can create accessibility to a lot of people with no SQL backgrounds. This idea was inspired by Salesforce's WikiSQL, which generates structured queries from natural language using Reinforcement Learning.</p>
    <hr>
    <h4 class="w3-opacity"><b>Word2vec</b></h4>
    <p>I was inspired by Google’s RankBrain – a Machine Learning system to embed vast amounts of natural language into mathematical vectors that the computer can understand. Google also produced the word2vec tool for computing continuous distributed representations of words. The goal of this research idea is to study how Natural Language Processing techniques compare in improving the results for a given search query.</p>
    <hr>
    <h4 class="w3-opacity"><b>BlockChain Technology</b></h4>
    <p>BlockChain has been gaining much traction in the past few years, especially with the meteoric rise of the digital payment system, bitcoin. I am motivated to investigate the idea of assigning a digital identifier to every asset, but at this point in time I plan to study the BlockChain technology in its entirety and learn about the academic research that already exists on cryptocurrency and digital payment systems.</p>
    <hr>
  </div>
</div>
